        # High Performance MySQL #

- Overview
    - Engine introduction

- DataType
    -

- Index Structure
    - Basis
        - B-Tree and B+Tree index in Mysql.
            - supports full-value, left prefix, column prefix, range and index matching, also order sorting.

        - Hash index in precise match. (Memory engine supports)
            - unable order sorting, partial matching and range matching, also have the conflict key issue.
            - can be useful in Star-schema.
            -
    - Clustered index

    - Non-Clustered index

    - Covering index

- Index Optimizing
    -
    - The tmp table created by sub-query has no index.

- Maintenance
    - Damaged table
        - command: CHECK TABLE can check out mostly flaw of table and index.
        - command: REPAIR TABLE or ALTER TABLE innodb_table ENGINE = InnoDB.
    - Update index stat
        - command: ANALYZE TABLE can re-generate statistics info.
        - Memory never store the statistics info.
        - MyISAM store it in disk, analyze operation would lock and scan the whole table.
        - InnoDB store it in memory, use command: SHOW INDEX FROM to show the cardinality of index.
            > InnoDB also store statistics in information_schema.statistics table.
            - InnoDB calculates the statistics index as opening the table at first time as well as analyze command.
    - Reduce index and data fragment
        - Row, Intra-row and Free space fragmentation would fall off performance.
        - command: OPTIMIZE TABLE or import/export to reorganize data.
        - InnoDB can reduce fragmentation by recreating index.
        - command: ALTER TABLE table ENGINE = InnoDB provides method to other engine not supports OPTIMIZE command.

- Query Optimizing
    - Mysql would return all matched result data set before calculating.
    - Useless data
        - The result set contains a lot of useless rows, use LIMIT to control.
        - INNER JOIN table USING(PRI_KEY) would return all result rows.
        > SELECT * always fetch all columns though some columns being not used.
        - The rows being queried repeatedly should be cached.
    - Extra records
        > Query overhead: response time, scanned rows and returned rows.
        - Better to create a proper index if not pick a Explain.type in selecting.
        - Reduce the rows to be scanned.
    - Refactor query
        - Single complicated query or multiple simpled queries.
        - Using LIMIT to delete a minor batch of data instead of the whole part.
        - Splitting associated query to multiple simpled queries.
            - The QueryCache can cached the result of each simpled query.
            - Improve query performance, reduce the contention of row locks, reduce the redundant rows.
    - Query procedure
        > Using EXPLAIN to analyze the query sql before writing.
        - Request to Mysql server
            - Half-duplex communication between server and client.
            - Avoid rudely disconnect and recommend to use LIMIT.
            - Mysql would release the resources of a query until all the queried result sending over.
            > Query state
                - Sleep: waiting for client request.
                - Query: executing query or sending back result to client.
                - Locked: waiting lock.
                - Analyzing and statistics: collecting engine stat and generating executive plan.
                - Copying to tmp table [on disk]: executing query and copying data to a tmp table. (GROUP/ORDER BY, UNION)
                - Sorting result: sorting the result set.
                - Sending data: transferring data between multiple states or generating data or sending data to client.
        - Find in QueryCache
            - A case-sensitive hash matching, authority validating then returning.
        - Parser and pre-handle the sql
            - Any error can interrupt the query.
            - Grammar Parser: Parse sql to a parsed-tree then validating authority.
        - Optimizer generates the executive plan
            - Transforming a parsed-tree to an executive plan.
            - Cost calculating: not including concurrent query, stored procedure or other custom function. (status: Last_query_cost)
            - Stop query: using LIMIT and found a impossible condition would stop the query in advance.
            - Value spread: like film.film_id > 5 and film_actor.film_id > 5.
            > IN(..) better than OR.
            - Hints
                - Providing some optimizing hints to the Optimizer
                - Hints in sql clause
                    - HIGH/LOW_PRIORITY, DELAYED, STRAIGHT_JOIN, SQL_SMALL/BIG/BUFFER_RESULT, FOR UPDATE, LOCK IN SHARE MODE
                    - USING/IGNORE/FORCE INDEX, SQL_CALC_FOUND_ROWS
                - Parameters in information_schema: optimizer_search_depth, optimizer_prune_level, optimizer_switch
        - Engine
            - Providing statistics data to Optimizer for generating executive plan.
            - Execute query by API based on the executive plan
            - Associated query
                - parse to a balance tree or left-depth-first tree.
                - Reverse associate order: INNER JOIN .. low cost, due to less nested loop and back tracking.
                - Natural associate order: SELECT STRAIGHT JOIN .. normal cost.
                - Greedy mode on if associated table number exceeded variable 'optimizer_search_depth'.
            - Sorting optimizing (filesort/merge)
                - Default order and sorting query might lead to a filesort.
                - Double data transfer: read rows from table and read rows from sorted rows. (old version)
                - Single data transfer: read rows and sort, then return. (after 4.1 and data less than 'max_length_for_sort_data')
                - Extra space could be assigned in sorting some table contains dynamic length columns. (VARCHAR/UTF-8)
                > Using filesort: an associated query only needs to sort the columns of the first table.
                > Using temporary and filesort: an associated query needs to sort crossed-table columns. (LIMIT end)
                > WHERE clause would execute before filesort.
            - Handler API in engine: generally create a handler instance for each table, the handler invoke the interfaces.
        - Return result
            - Begin to sending result back to client once the server generated the first row result.
        - Sub-query optimizing
            - Using EXPLAIN to measures a sub-query is better or worst, comparing between the sub-query and associated query.
        - UNION
            - Original: (SELECT * FROM A) UNION ALL (SELECT * FROM B) LIMIT 20; (union two full tables)
            - Optimized: (SELECT * FROM A LIMIT 20) UNION ALL (SELECT * FROM B LIMIT 20) LIMIT 20; (limit then union)
            > Mysql always using tmp table to process UNION (ALL), applying DISTINCT to tmp table if is UNION ALL.
        - Loose index probe
            - SELECT actor_id, MAX(film_id) FROM film_actor GROUP BY actor_id;
            - Extra = Using index for group-by.
        - Update and query in same table
            - Original: UPDATE table AS outer SET cnt = (SELECT count(*) FROM table AS inner WHERE inner.type=outer.type).
            - Instead: UPDATE table INNER JOIN(SELECT type, count(*) AS cnt FROM table GROUP BY TYPE) AS der USING(type) SET table.cnt=der.cnt;
    - Optimizing special type query
        - COUNT(): * refers to counting all the rows including NULL columns, other excluding.
            > COUNT(*) are more excellent than range-COUNT(*) in MyISAM.
            - Classification count: SELECT SUM(IF(col=v1,1,0)), SUM(IF(col=v2,1,0)) .. FROM table.
            - Approximation: execute an EXPLAIN sql to fetch the approximated count.
            - More complicated cases are recommend to add a statistics table.
        - GROUP BY and DISTINCT
            - Index is the most efficient way to optimize this type query.
            - The tmp table or filesort could be used to process GROUP BY if no index available.
            - Hints: SQL_SMALL/BIG_RESULT
            > Can optimize the query by ORDER BY NULL if you don't need the ordered result in a GROUP BY query sql.
            - Using WITH ROLLUP to take a super clustering might improve query performance.
        - LIMIT
            - Also index is the most convenient way to optimize.
            - LIMIT 1000,20 would ask the engine to scan 1000 rows before reaching the target rows and discard after return.
            - Using INNER JOIN to optimize LIMIT and LIMIT with ORDER BY.
            - Using BETWEEN to optimize LIMIT query in certain range.
            - Marking a savepoint before LIMIT query.
        - Using user-custom variable
            - User-custom variable is a tmp storage for storing data and it exists in current connection with Mysql.
            - Using custom variable would not allow to use QueryCache.
            > Variable would across different business if a connection belongs to a DataSource or ConnectionPool.
            > Make sure the assignment and fetch of the variable should occur in the same phase.
            - SET @var := value, defines a variable. (use := to avoid ambiguity)
            - Update and return column in natural network state.
                - Original: UPDATE table SET lastUpdated = NOW() WHERE id = 1; SELECT lastUpdated FROM table WHERE id = 1;
                - Optimized: UPDATE table SET lastUpdated = NOW() WHERE id = 1 AND @now := NOW(); SELECT @now;
        - SELECT FOR UPDATE
            - Using CONNECTION_ID() in UPDATE sql to implement the functionality like SELECT FOR UPDATE.

- Advanced Features
    - Partition
        - command: PARTITION BY
        - Partition table working well in large table, but it has no overall index.
        - Requiring imports all the primary key and unique key if the key in partition contains the primary or unique key.
        - Unable to use foreign key in partition table, max partition is 1024 in one table.
        > Using partition in condition that the engine can filtering most partitions to fetch the target one.
        - All partitions are invisible to client, they requires a identical store engine.
        > Operation in partition table
            - SELECT: open and lock all partitions, then filter.
            - INSERT: open and lock all partitions, then selecting the target partition and do insert .
            - DELETE: like INSERT operation, do delete at the end.
            - UPDATE: like INSERT operation, do update at the end.
        - Partition type: by range, key, hash and list.
        - BigData table partition strategy: scanning full partition with no index or separating hot-data with index.
        > Partition creation strategy
            - Lots of data would storing in the first partition if the value of partition column is NULL.
            - Create a 'useless' partition as the first partition to avoid NULL value invalid the partition.
            - Better use a indexed column as partition key, otherwise the sql operations might be scanning all partitions.
            - Be awareness of the targeting and locking cost of partitions.
            - Refactor the partition can use the method as refactor the table: create new, insert data then delete old.
        > Optimizing query in partition
            > Analyze: EXPLAIN PARTITION SELECT ..
            > The optimizer can filter the partition if the partition column being used.
            > Using function on partition column would making the partition filtering unavailable, even a partition function.
                - WHERE YEAR(day) = 2021 should be optimized to WHERE day BETWEEN '2021-01-01' AND '2021-12-31'.
        - Merge table (deprecated)
            - Create tables and using MRG(..) to merge then into a combined table.
            - Drop the merged table would makes no impact to sub-tables, but delete sub-table could bring problems.
    - View
        > View is a tmp table with no data storing, has the same naming space of table.
        > Mysql would using TEMPTABLE algorithm if any GROUP BY, DISTINCT, UNION, sub-query and aggregate function in sql.
            - EXPLAIN select_type would show DEPRIVED in TEMPTABLE algorithm.
        - CREATE VIEW .. WITH CHECK OPTION enable a view to updated the relational table while matching WHERE condition.
            - Invalid: UPDATE Oceania SET Continent = 'Atlantis';
            - TEMPTABLE view can not be updated.
    - Storing code
        - Storing code for Storage Procedure, Trigger, Event and Function.
        - Storage Procedure
            - Advantage: low network latency, code reuse, improve security and without external dependency.
            - Disadvantage: no debug mode, functional limited, resource consuming and is hard to troubleshooting in slow-log.
        - Trigger
            - It have no return value but only read and change data.
            - Only one Trigger for each event in each table. (AFTER INSERT ..)
            > Only supports row trigger for one row, may fall down performance in large data.
            - Hard to troubleshooting, potential lock waiting and dead lock problems.
            > Trigger fails, sql fails.
            > Using Trigger as foreign key restriction may fetching dirty data in concurrent condition. (correct: SELECT .. FOR UPDATE)
        - Event can be used like schedule task and it was recorded in INFORMATION_SCHEMA.EVENTS.
    - Foreign Key can be replaced by Trigger.
    - Cursor is read-only, unidirectional and storing in tmp table.
    - Prepared Statement
        - Parse a sql to a repeatable executive plan and caching it, receive data from client for sql data binding.
        - Connection isolation, careful the variable resource to avoid resource leak.
    - Charset & Collation
        - The collation impacts the char columns and the sorting order.
        > Setting in creation
            - Create schema with collation based on variable 'character_set_server'.
            - Create table with collation would based on schema's charset and collation if no specified.
            - Create column with collation would based on table's charset and collation if no specified.
        > Setting in connection
            - The client-end has different charset with server-end, server-end would take a necessary charset conversion.
            > Converting received data from 'charset_set_client' to 'character_set_connection', finally return data of 'character_set_result'.
            > Make sure both ends are set to identical charset.
            > A great way to setting charset is USE database, SET NAMES charset then loading data.
            - command: SHOW CHARACTERSET and SHOW COLLATION to show charset and collation mysql supports.
            > Index would not work while using a charset different with the server setting.
            > Choosing a proper charset based on what data you are going to storing.
    - Full-text Index
        - FULLTEXT KEY `..` (..) to create a full-text index in a column. (SHOW INDEX FROM table)
        - Full-text index include all columns and it would not record which column the keyword comes from.
        > Using function WHERE MATCH(columns) AGAINST(content) in full-text search.
        > Requires the dataset and index were loaded into cache to perform fast.
        - Generates more fragment and leads to more OPTIMIZE TABLE operations.
        > Natural Language search
            - Return auto-sorted result based on text relevance.
            - Mysql would using filesort if found MATCH(columns) in ORDER BY or GROUP BY.
        - Boolean full-text search
            - Requires the length of keyword greater than 'ft_min_word_len' and less than 'ft_max_word_len'.
            - Using special modifier(~-+*) to custom search and it returns unsorted result.
            > Filtering dataset by index and return some records, then match the keyword again on returning records.
                > It would perform slowly if the first filtering operation return the amount of records almost equals to total.
            - MATCH(..) AGAINST('+factory +casualties' IN BOOLEAN MODE)
            - MATCH(..) AGAINST('"spirited casualties"' IN BOOLEAN MODE) means precise Phrase Search. (slow)
            - Combine WHERE condition in AGAINST(..)
    - XA Transaction
    - Query Cache
        - Do not open query cache if you don't know how to make it works well.
        - Sub-query and storage procedure can not accessing the query cache.

- Configuration
    - Principle
        - From command args and configuration path: /etc/my.cnf or /etc/mysql/my.cnf
        > The first action to optimizing is focus on SCHEMA not the server configuration.
        > Can set a default value to a config entry by command SET key = DEFAULT;
        - Some dynamic configs can be modified in server running. (like 'sort_buffer_size')
        - Certain side effect could be made if modified configs in running, do what you really known.
        > Recover the config back to DEFAULT after modified it if the situation really requires.
        - Adjusts the parameter's value based on test cases.
        - Don't trust those rates easily, like the relationship of cache hit rates and capacity.
        - Do not use optimizing scripts applying on the server.
    - Customize
        > Most significant settings of InnoDB are: 'innodb_buffer_pool_size' and 'innodb_log_file_size'.
        > Buffer Pool and Log File capacity
            > Buffer Pool caching the index, data page, hash index, insert buffer, lock and so on.
            > Dirty Page in Buffer Pool, modified and waiting to be flushed back to disk.
            - Choosing the proper capacity for buffer pool like 87.5% of total memory capacity.
        - Checking 'open_files_limit' setting in unix-like server. (for connection amount)
        > The maximum capacity of Mysql is subjected to the system like 32-bit and 64-bit.
        > Setting a proper size for concurrent connection based on peak and normal amount.
            - Unix command: top(VIRT column) or ps(VSZ column).
        - MyISAM Key Caches see variable 'key_buffer_size'
            - SUM(INDEX_LENGTH) FROM INFORMATION_SCHEMA.TABLES to check total index length.
    > InnoDB Tx Log and Log buffer
        > Log buffer has no necessary to set a huge capacity value, 1 to 8MB should be enough.
        > First write the update operation into buffer and waiting to meet the following condition to write into Log file.
            - when log buffer full, transaction committing and every second.
        > log buffer flush control setting: 'innodb_flush_log_at_trx_commit'.
            - 0 flush log buffer into OS(fileSystem) buffer and disk on every second, ignore transaction commitment.
            - 1 default, flush into OS buffer and disk on every transaction commitment.
            - 2 flush into OS buffer every transaction commitment.
        - Log buffer Mutex contention could be the performance bottleneck.
        - Collecting transaction IO operations in log, turn random IO to sequential IO and flush into disk at certain checkpoint.
        > Log file size setting: 'innodb_log_file_size' and 'innodb_log_files_in_group'.
    > Doublewrite Buffer
        > Avoid data corrupted by page write operation not finished.
        - InnoDB page size: 'innodb_page_size' the batch size to write back to disk.
        > First write page into doublewrite buffer, and then write into disk.
            - Original page in disk would have no impacts if corrupting in doublewrite buffer writing.
            - Fetching page data from doublewrite buffer if corrupting in disk writing.
            - Checksum in every page tail is used to valid the completion of a page.
    > Concurrency in Mysql
        > Basic setting to control concurrency: 'innodb_thread_concurrency'. (0-unlimited)
        > Two phase process to control concurrent connection
            - Thread entering a sleep status in 'innodb_thread_sleep_delay' seconds if the connection exceed concurrent amount.
        - The last protective control setting: 'max_connections'.
    - Avoid endless log: 'expire_logs_days' to clean up expired logs.

- Replica
    - Mysql supports different types of synchronization.
        - Based on replicating events from the source's binary log.
            - Statement Based Replication(SBR) replicates entire sql statements.(some function, procedure .. unavailable)
            - Row Based Replication(RBR) replicates only the changed rows, DEFAULT.
            - Mixed-format logging replicates by SBR or RBR, depends on which is most appropriate.
        - Based on global tx identifiers which means not require working with log files.
    > Synchronize the binary log of master to relay log, then replaying based on relay log.
    - Backup: close master to replicates data to replica, using mysqldump and copy snapshot or backup files to replica.
    - An replica can be the master of the other replicas by updating its binary logs.('log_slave_updates')
    - Replica Filter can filtering data at replicating during replicating from master and relay log.
    - Use a server with blackhole engine tables as a distributing server.